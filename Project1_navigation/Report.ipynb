{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc2ae49",
   "metadata": {},
   "source": [
    "# Project 1: Navigation\n",
    "This is a project report for a navigation task using DQN. The agent is tasked to collect as many yellow bananas as possible while avoiding blue bananas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a32347b",
   "metadata": {},
   "source": [
    "### Enviroment\n",
    "The project is based on a Unity enviroment, where an agent is travelling in a workspace to collect bananas. Every time the agent collects a yellow banana, it gets a reward of +1 and every time it collects a blue banana, it receives a reward of -1. Therefore, the goal for this project is to train the agent so that it collects as many yellow bananas as possible while avoiding blue bananas. The environment is considered solved if the agent get an avarage score of +13 over 100 consecutive episodes.\n",
    "\n",
    "<img src=\"banana.gif\" />\n",
    "\n",
    "The size of the state space is 37, which contains the agent's velocity, as well as ray-based perception of objects. The size of the action space is 4, indicating 4 directions the agent can move (forward, backward, left, right)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63545f",
   "metadata": {},
   "source": [
    "### Algorithm Overview\n",
    "A value-based [Deep Q-Network (DQN)](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning) has been used to solve this navigation task. A typical Deep Q-Network is built on a model-free reinforcement learning algorithm Q-learning, whcih learns the value of an action in a particular state. In Q-learning, a Q-table is generated to store the value of each state-action pair. Such tabular methods are good to solve problems where the size of the state and action space is relatively small. For problems with larger or continuous state/action space, a deep neural network has been used which takes a state as the input, and predicts all the action values for that state. The loss is computed by comparing the network prediction with that comes from the Q-learning so that it learns a Q-table approximation. <br/>\n",
    "\n",
    "There are two major improvements in DQN\n",
    "- Experience Replay\n",
    "- Fixed Q-Targets\n",
    "\n",
    "The experience replay is introduced so as to alleviate the correlation between different experience tuples and Fixed Q-Targets is introduced to decrease the instability arising from parameter optimization during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b008bd24",
   "metadata": {},
   "source": [
    "### Code Overview\n",
    "The code is modified based on a mini-project \"Lunar Lander\" in the same course. The code contains\n",
    "- ##### model.py\n",
    "  which specifies the structure of the neural network. The structure is as follows. <br/>\n",
    "  state (input) -> fully-connected layer (37, 64) -> ReLu -> fully-connected layer (64, 64) -> Relu -> fully-connected layer (64, 4) -> action values (output).\n",
    "- ##### dqn_agent.py \n",
    "  which defines the agent's behaviors (choose epsilon-greedy action, add experience tuples to the replay buffer, and train the model with batches of experience tuples). The buffer size is set to be large enough ($\\small 10^5$) and the batch size to be 64. The disaccount is 0.99 and the learning rate is $\\small 5 \\times 10^{-4}$.\n",
    "- ##### Navigation.ipynb\n",
    "  which initializes the enviroment for the agent to interact with and trains the agent using DQN. The maximum number of training episodes is 2000 and the maximum number of time steps per episode is 1000. For epsilon, the start value is 1.0 and the minimum value is 0.1. The decay rate of epsilon value is set to be 0.995. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b192d",
   "metadata": {},
   "source": [
    "### Result\n",
    "The environment is considered solved if the agent get an avarage score of +13 over 100 consecutive episodes. It takes 388 episodes in my case to solve the problem. Below is the statistics in the training process.\n",
    "Episode 0 \t     Average Score: 0.00 <br/>\n",
    "Episode 100 \t Average Score: 1.17 <br/>\n",
    "Episode 200 \t Average Score: 4.92 <br/>\n",
    "Episode 300 \t Average Score: 8.38 <br/>\n",
    "Episode 400 \t Average Score: 10.40 <br/>\n",
    "Episode 488 \t Average Score: 13.02 <br/>\n",
    "Environment solved in 388 episodes!\tAverage Score: 13.02 <br/>\n",
    "\n",
    "Below is the plot showing the score for each episodes, as well as the average score of last 100 episodes per episode.\n",
    "<img src=\"episode_scores.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a989c",
   "metadata": {},
   "source": [
    "### Future Improvements\n",
    "There are variants of DQN which can potentially improve the agent's performance, such as Double DQN and Dueling DQN. Improvements can also take place in terms of how to prioritize experience replay to learn from the most important experience tuples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
