{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "019d068f",
   "metadata": {},
   "source": [
    "# Project 2: Continuous Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316102d",
   "metadata": {},
   "source": [
    "### Project description\n",
    "The folder contains the codes and a report for project 2 of the Deep Reinforcement Learning nanodegree. The project is based on a Unity ML-Agents Reacher environment, where a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "<img src=\"reacher.gif\" />\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.\n",
    "\n",
    "In this project, I choose the **Deep Deterministic Policy Gradient (DDPG)** algortithm, an actor-critic algorithm for learning continous actions to train 20 identical agents in the Unity environment. The agents must get an average score of +30 (over 100 consecutive episodes, and over all agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13a74d",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "The **Deep Deterministic Policy Gradient (DDPG)** algortithm is effective in solving problems with continuous action space. The pseudocode is demonstrated as follows from the [paper](https://arxiv.org/pdf/1509.02971.pdf)\n",
    "\n",
    "<img src=\"pseudocode_ddpg.png\" />\n",
    "\n",
    "It contains a critic network, as well as an actor network, each of which has its own local and target network. It uses Experience Replay and slow-learning target networks from DQN, and it is based on DPG, which can operate over continuous action spaces. \n",
    "- For updating the critic, it is similar to DQN except that it uses target actor network to produce the next action in calculating the Q_target ($y_i$).\n",
    "- For updating the actor, it performs gradient ascent on the sampled policy gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd861ca",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "The codes contain 3 main files:\n",
    "- `model.py`: defines and specifies the Actor and Critic network. (This part builds on top of the [Udacity repository](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal))\n",
    "- `ddpg_agent.py`: implements the DDPG training process, as well as defining the action noise and the replay buffer. (This part builds on top of the [Udacity repository](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal))\n",
    "- `Continuous_Control.ipynb`: the notework to train the agent. Run cell by cell\n",
    "\n",
    "#### Network architecture\n",
    "- Actor Network\n",
    "```\n",
    "input layer (33 units, state_space_size)\n",
    "hidden layer 1 (fully connected 256 units, batch_norm, relu)\n",
    "hidden layer 2 (fully connected 128 units, relu)\n",
    "output layer (4 units, tanh, action_space_size)\n",
    "```\n",
    "- Critic Network\n",
    "```\n",
    "input layer (33 units, state_space_size)\n",
    "hidden layer 1 (fully connected 256 units, batch_norm, relu)\n",
    "hidden layer 2 (fully connected 128 units, relu)\n",
    "output layer (1 units, Q_value)\n",
    "```\n",
    "\n",
    "#### Hyperparameters for training\n",
    "```\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0   # L2 weight decay\n",
    "EPSILON_DECAY = 0.9999  # decay rate\n",
    "LEARNING_TIMES = 10     # the number of times for learning at a single phase.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece21730",
   "metadata": {},
   "source": [
    "### Results and key notes\n",
    "The statistics for the training process and the plot of the experimental results are demonstrated below.\n",
    "```\n",
    "Episode 10 \t Average Score: 2.33\n",
    "Episode 20 \t Average Score: 4.77\n",
    "Episode 30 \t Average Score: 5.87\n",
    "Episode 40 \t Average Score: 6.95\n",
    "Episode 50 \t Average Score: 8.17\n",
    "Episode 60 \t Average Score: 9.00\n",
    "Episode 70 \t Average Score: 10.06\n",
    "Episode 80 \t Average Score: 10.98\n",
    "Episode 90 \t Average Score: 11.78\n",
    "Episode 100 \t Average Score: 12.71\n",
    "Episode 110 \t Average Score: 14.91\n",
    "Episode 120 \t Average Score: 17.00\n",
    "Episode 130 \t Average Score: 19.21\n",
    "Episode 140 \t Average Score: 21.66\n",
    "Episode 150 \t Average Score: 24.21\n",
    "Episode 160 \t Average Score: 26.78\n",
    "Episode 170 \t Average Score: 29.00\n",
    "Episode 176 \t Average Score: 30.13\n",
    "Environment solved in 76 episodes!\tAverage Score: 30.13\n",
    "```\n",
    "<img src=\"plot.png\" />\n",
    "\n",
    "The training process goes good. In the image above, it is specified that environment is solved in 76 episode.\n",
    "The actual episode at which environment is solved (+30 over last 100 episodes) is 176.\n",
    "\n",
    "#### Key notes\n",
    "Initially I could not get the agents training effectively, no matter how I tuned the hyperparameters. The trend was, it started with some evidences of learning, with the first 3-5 episodes. It got scores around 1.0 but after that the scores stuck there and not increased. I searched online and found another source of DDPG pseudocode [here](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#pseudocode). The line 9-10 caught my attention which said \"if it's time to update then...\". It gave the hint that it might not be a wise idea to training every time an experience tuple is added to the replay buffer. Learning too frequently with very minor change of the current buffer might lead to local optimum. The statistics that I initially received gave me a strong indication that I encountered the local optimum after a few episodes due to learning too frequently. So I adjusted my training process by only triggering the learning every 20 time steps, so as to allow the agents to update the buffer more before each training. It turned it worked. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f015f3",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "It is mentioned that the second version (multi-agent version) is useful for algorithms like PPO, A3C, and D4PG that use multiple (non-interacting, parallel) copies of the same agent to distribute the task of gathering experience. So the future work can include the implementation of PPO, A3C and D4PG."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
